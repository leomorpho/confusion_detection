{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ca-5opg4Js2L"
   },
   "source": [
    "## TODO:\n",
    "Change functions names if you want to\n",
    "- `stitch_datasets(path_to_labels_jsons, path_to_open_pose_jsons)`: stitch the labels (confused, unknown, not confused, no test subject) with the OpenPose data extracted for each frame (about 100 features of body joints).\n",
    "  - Input:\n",
    "    - Path to the labels jsons\n",
    "    - Path to the OpenPose features jsons\n",
    "  - Returns:\n",
    "    - Dataframe where each row has a label and the OpenPose features\n",
    "\n",
    "- `preprocess(dataframe)`: give unit variance and zero mean to our data. \n",
    "  - Some pre-processing steps will be part of the `stitch_datasets` function\n",
    "    - Remove rows with no OpenPose feature data\n",
    "\n",
    "- `split_data(dataframes)`: Sklearn has a function for that. I'm guessing TensorFlow does as well.\n",
    "  - Input:\n",
    "    - Dataframe of the whole dataset\n",
    "  - Returns:\n",
    "    - `train_data`: a training dataset\n",
    "    - `test_data`: a testing dataset\n",
    "\n",
    "- `create_model(train_data)`: this is where the fun happens. Let's build a NN!\n",
    "\n",
    "- `test_model(test_data)`: run a cross validation test, or whatever test is appropriate and usually used for NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXONSYt24bKQ"
   },
   "source": [
    "## NN architecture\n",
    "[The mostly complete chart of Neural Networks, explained](https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464)\n",
    "\n",
    "We want:\n",
    "- Some temporality\n",
    "\n",
    "Arcitectures to start from:\n",
    "- RNN: Recurrent Neural Network\n",
    "  - Simple model with context preservation\n",
    "- LSTM: Long / Short Term Memory\n",
    "  - More complicated than RNN, but better for sequences (I think?)\n",
    "- GRU: Gated Recurrent Unit\n",
    "  - Similar to LSTM but less ressource intensive and similar performance\n",
    "- AE / VAE: Auto-Encoder / Variational Auto-Encoder\n",
    "  - Good at classfying patterns. Seems to be more for unsuperverised learning, which is not what we're after. I wonder if it can do logistic classification. Perhaps wer could generate stick figures of confused people. A little off topic though.\n",
    "- SVM: Support Vector Machine\n",
    "  - Can be used as a baseline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvYhGcFL1Xws"
   },
   "source": [
    "## Download our dataset from our Dropbox and our main dependencies\n",
    "Make sure to add all newly created data to the `combined_jsons` folder in our shared dropbox. The data will be directly curled from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPynH3BYQ8A9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "REDOWNLOAD = True\n",
    "\n",
    "if REDOWNLOAD:\n",
    "  shutil.rmtree(DATA_DIR)\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  !curl -L -o data.zip https://www.dropbox.com/sh/3ty3gszbpexan9q/AAC4F7GnYk-o0CU-HvM29sd9a?dl=0\n",
    "  !unzip data.zip -d data\n",
    "  !rm data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Vh1vJwa4qGm"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OtK_Uvwi3bns"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from typing import *\n",
    "import json\n",
    "\n",
    "dataset_paths = glob(f\"{DATA_DIR}/*\")\n",
    "\n",
    "# Create single list object from all the JSONs\n",
    "raw_sequences: List[List[List[float]]] = []\n",
    "for path in dataset_paths:\n",
    "  with open(path, \"r\") as f:\n",
    "    raw_sequences.append(json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZddK8WY5ILW"
   },
   "source": [
    "Get centroid for every frame. If the centroid differs widely between 2 frames, it may indicate that different people were picked up by OpenPose.\n",
    "Centroid code taken from [here](https://stackoverflow.com/questions/23020659/fastest-way-to-calculate-the-centroid-of-a-set-of-coordinate-tuples-in-python-wi).\n",
    "\n",
    "Format of OpenPose output can be found [here](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md). A frame is represented by `x1,y1,c1,x2,y2,c2,...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWXnNNXN5Jwc"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "MAX_DROPPED_FRAMES = 3\n",
    "MIN_DISTANCE_BTW_POSITIONS = 0.001\n",
    "\n",
    "def centeroidnp(arr):\n",
    "  \"\"\"\n",
    "  Find the centroid for an array of (x, y) points\n",
    "  \"\"\"\n",
    "  length = arr.shape[0]\n",
    "  sum_x = np.sum(arr[:, 0])\n",
    "  sum_y = np.sum(arr[:, 1])\n",
    "  return sum_x/length, sum_y/length\n",
    "\n",
    "def dist(pair1: Tuple[float, float], pair2: Tuple[float, float]) -> float:\n",
    "  \"\"\"\n",
    "  Return distance between 2 points as a single scalar\n",
    "  \"\"\"\n",
    "  x_dist = abs(pair1[0] - pair2[0])\n",
    "  y_dist = abs(pair2[1] - pair2[1])\n",
    "\n",
    "  return (x_dist + y_dist) / 2\n",
    "\n",
    "# new sequences are the cleaned up sequences. If less than MAX_DROPPED_FRAMES \n",
    "# were dropped, the sequences will be stiched, and new_sequences will contain\n",
    "# a single sequence.\n",
    "new_sequences = [[]]\n",
    "\n",
    "for raw_sequence in raw_sequences[:1]:\n",
    "\n",
    "  # Keep track of the number of new sequences created\n",
    "  sequences_count = 0\n",
    "\n",
    "  # Keep track of centroid from last frame\n",
    "  last_frame_centroid = None\n",
    "  # If less than MAX_DROPPED_FRAMES are dropped, the sequences will be stitched.\n",
    "  num_dropped_frames = 0\n",
    "\n",
    "  for frame in raw_sequence[:1]:\n",
    "    # Create new sequence if the maximum amount of dropped frames was reached \n",
    "    # or if this frame is the first in the sequence\n",
    "    if num_dropped_frames >= MAX_DROPPED_FRAMES or not last_frame_centroid:\n",
    "      new_sequences.append([])\n",
    "      sequences_count += 1\n",
    "      new_sequences[sequences_count].append(frame)\n",
    "    else:\n",
    "      # Create 2D array\n",
    "      frame = frame[1:]\n",
    "      frame_positions = list(zip(frame,frame[1:]))[::2]\n",
    "      current_centroid = centeroidnp(np.array(frame_positions))\n",
    "      print(current_centroid)\n",
    "      print(last_frame_centroid)\n",
    "      if dist(current_centroid, last_frame_centroid) > MIN_DISTANCE_BTW_POSITIONS:\n",
    "        # Current and previous frames are likely of the same participant\n",
    "        new_sequences[sequences_count].append(frame)\n",
    "      else:\n",
    "        # This is a dud. Don't use it.\n",
    "        num_dropped_frames += 1\n",
    "\n",
    "    # Update position of last frame to current frame\n",
    "    last_frame_centroid = current_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7Z-9obS8xgv"
   },
   "source": [
    "If a dropped frame is between 2 valid sequences, consider stitching them back together.\n",
    "* Drop frames which have **no subject** in them. \n",
    "* Drop frames which have **wrong** subject in them. \n",
    "* Give unit variance (and zero mean?) to all points.\n",
    "\n",
    "Stitch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsYRAU0LEPnY"
   },
   "source": [
    "## Resources\n",
    "### Educational\n",
    "* [DeepMind deep learning lectures](https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF)\n",
    "### Annotation tools\n",
    "* [List of open source solutions](https://www.simonwenkel.com/2019/07/19/list-of-annotation-tools-for-machine-learning-research.html)\n",
    "* [opencv/cvat](https://github.com/opencv/cvat)\n",
    "* [alexandre01/UltimateLabeling](https://github.com/alexandre01/UltimateLabeling)\n",
    "### Existing trained models\n",
    "* [onnx/models](https://github.com/onnx/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8kpbdaCEslu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "confusion_detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
